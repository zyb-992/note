# 集群

1. **节点**

   1. **节点：一个运行在集群模式下的Redis服务器**

   2. 一个集群中包含多个节点 节点可以分为主节点从节点 

      1. 主节点进行处理槽
      2. 从节点进行对主节点的复制 

   3. 连接其他服务器作为集群节点

      ```
      CLUSTER MEET <ip> <port>
       
      # 例如当前客户端连接的服务器端口为7000
      # 执行命令
      CLUSTER MEET 127.0.0.1 7001
      
      # 此时7001服务器作为一个节点加入到7000的集群中
      
      # 查看集群节点信息
      CLUSTER NODE 
      ```

      

   4. 启动节点：Redis服务器启动时根据配置文件中的cluster-enabled选项来决定是否开启服务器的集群模式

      ![image-20220729181932914](C:\Users\zyb\AppData\Roaming\Typora\typora-user-images\image-20220729181932914.png)

      

   5. **集群节点的数据结构**

      

      ![image-20220729182042966](C:\Users\zyb\AppData\Roaming\Typora\typora-user-images\image-20220729182042966.png)

      

   6. CLUSTER MEET命令实现

      1. 客户端连接到节点A 通过向A发送该命令 
      2. A收到命令后会向B进行握手 来确认是否存在
      3. A为B创建并维护一个clusterNode结构 将该结构实例添加到A自身的clusterState的**nodes**字段下面
      4. 同时节点B也会为A创建并维护一个clusterNode结构 将该结构实例添加到B自身的clusterState的**nodes**字段下面
      5. B发送一条PONG消息给A 表明B成功接收了A的MEET消息
      6. A接收到PONG后 再次发送PING消息
      7. B接收到PING后 发回PONG消息

      ![image-20220729182519393](C:\Users\zyb\AppData\Roaming\Typora\typora-user-images\image-20220729182519393.png)

2. **槽指派**

   1. **Redis集群通过分片的方式来保存数据库中的键值对 **

   2. **集群中的整个数据库被分为16384个槽slot  数据库每个键属于16384个槽中的其中一个 **

   3. 集群每个节点可以处理0-16384个槽

   4. **当每个槽都有对应节点处理时 集群才会处于上线状态 / 只要有一个槽没有被处理 集群就一直处于下线状态**

   5. **记录节点的槽指派信息**

      1. 每个节点clusterNode结构的**slots数组**记录了自身的节点负责处理哪些槽

      2. 每个节点除了会将自己负责处理的槽记录在clusterNode结构的slots数组中 还会**将自己的slots数组通过消息发送给集群中的其他所有节点 以此来告知其他节点自身正在处理哪些槽** 

         当节点A通过消息从节点B那里接收到节点B的slots数组时，节点A会在自己的**clusterState.nodes字典**中查找节点B对应的**clusterNode**结构，并对结构中的slots数组进行保存或者更新

         ![image-20220729221755282](C:\Users\zyb\AppData\Roaming\Typora\typora-user-images\image-20220729221755282.png)

      3. 记录集群所有槽的指派信息

         1. **clusterState结构中的slots数组**记录了集群中所有槽的指派信息
         2. 数组中每个元素项都指向一个clusterNode结构的指针
         3. 如果slots[i]指向Null 那么槽i未指派给任何节点
         4. 如果slots[i]指向一个clusterNode结构 那么槽i已经指派给了clusterNode结构代表的节点

         ![image-20220729222345736](C:\Users\zyb\AppData\Roaming\Typora\typora-user-images\image-20220729222345736.png)

      4. `CLUSTER ADDSLOT`命令的实现

         1. 先遍历所有输入槽 检查是否都是未指派槽（clusterNode.slots数组所有二进制位都是0

            ![image-20220729222840778](C:\Users\zyb\AppData\Roaming\Typora\typora-user-images\image-20220729222840778.png)

         2. 再次遍历输入所有槽 将这些槽指派个当前节点

         3. 指派后 **clusterState.slots数组在这些槽中指向的都是节点clusteNode自身 且clusterNode.slots数组对应的位被置为1**

            ![image-20220729222854142](C:\Users\zyb\AppData\Roaming\Typora\typora-user-images\image-20220729222854142.png)

         

3. **在集群中执行命令**

   1. 每个槽都进行指派后 集群进入上限状态 客户端能够对集群中的节点发送数据命令

   2. 接受命令的节点需要计算出命令要处理的数据库键处于哪个槽 是否指派给了自己

      ![image-20220729223343306](C:\Users\zyb\AppData\Roaming\Typora\typora-user-images\image-20220729223343306.png)

   3. **MOVED错误**

      ```
      MOVED <slot> <ip>:<port>
      
      # 表示槽12586由Ip为127.0.0.1 端口为7002的集群节点（服务器）负责
      MOVED 12586 127.0.0.1:7002
      ```

      1. 客户端接收到节点返回MOVED错误 会跟据提供的信息 redirect到正确的节点 并向该节点重新发送之前需要执行的命令

         ![image-20220729230111954](C:\Users\zyb\AppData\Roaming\Typora\typora-user-images\image-20220729230111954.png)

         ![image-20220729230133347](C:\Users\zyb\AppData\Roaming\Typora\typora-user-images\image-20220729230133347.png)

   4. 计算键属于哪个槽：使用CRC16校验和

      ```
      def slot_number(key)
      	return CRC16(key) & 16383
      	
      # 查询键的槽号
      CLUSTER KEYSLOT key
      ```

   5. 判断槽是否由当前节点负责处理

      1. 节点计算出键所属槽后 会检查自己在**clusterState.slots数组**中的项i 看**slots[i]指向的clusterNode结构**是否是**clusterState.myself **
         1. 是 则当前节点可以执行客户端发送的命令
         2. 若不是 **节点会根据slots[i]指向的clusterNode结构所记录的ip和端口号** 向客户端返回**MOVED错误 指引客户端转向正在处理槽i的节点**

   6. 节点数据库的实现

      1. 节点只能使用0号数据库 而单机服务器可以使用0-15号数据库
      2. 保存键值对以及键值对过期时间的方式 与单机Redis服务器的方式完全相同

      

4. **重新分片**

   1. **Redis集群的重新分片操作可以将任意已经指派给某个源节点的槽改为指派给另一个节点（目标节点）并且相关槽所属的键值对也会从源节点移动到目标节点**

   2. 重新分片操作可以在线运行 集群不需要下线 并且源节点和目标节点可以在重新分片过程中处理命令请求

   3. **实现原理**

      1. 重新分片操作是由Redis的集群管理软件redis-trib负责执行的

      ![image-20220729231702330](C:\Users\zyb\AppData\Roaming\Typora\typora-user-images\image-20220729231702330.png)

      ![](C:\Users\zyb\AppData\Roaming\Typora\typora-user-images\image-20220729231352120.png)

      

5. **ASK错误**

   1. 重新分片期间 若源节点向目标节点迁移槽的过程中 客户端向源节点取某个键 而这个键所在的槽刚好是已经迁移到目标节点情况下 此时**源节点向客户端返回一个ASK错误** 指引客户端转向正在导入槽的目标节点 并再次发送之前需要执行的命令

   2. 节点的clusterState结构的imorting_slots_from数组记录了节点正在从其他节点(源节点)导入的槽

      1. 若imorting_slots_from[i]的值不为NULL 而是指向某个clusterNode结构 那么表示当前节点正在从该结构代表的节点导入槽i

      2. 例如

         ```
         # 当前节点7002 
         CLUSTER SETSLOT 16198 IMPORTING run_id(7003)
         
         # 表示7002将导入7003的16198槽 并将16198槽的使用权交付给7002
         ```

      ![image-20220729232609698](C:\Users\zyb\AppData\Roaming\Typora\typora-user-images\image-20220729232609698.png)

   3. 节点的clusterState结构的**migrating_slots_to**数组记录了当前节点正在将槽迁移至其他节点

      1. migrating_slots_to[i]的值指向一个clusterNode结构 表示当前节点槽正在将槽i迁移至clusterNode所代表的节点

         ```
         # 当前节点7002 runid为7003
         CLUSTET SETSLOT 16198 MIGRATING runid
         
         # 表示7002正在将16198槽的使用权交给7003
         ```

      ![image-20220729232825045](C:\Users\zyb\AppData\Roaming\Typora\typora-user-images\image-20220729232825045.png)

   4. **ASK错误原理**

      1. 若节点没有在自己的数据库中找到键key 那么节点会检查自己的**clusterState结构的migrating_slots_to[i]**  看key所属的槽i是否正在进行迁移 若正在迁移 则返回客户端一个ASK错误 并引导客户端到长在导入槽i的节点查找key

         ![image-20220729233151781](C:\Users\zyb\AppData\Roaming\Typora\typora-user-images\image-20220729233151781.png)

         ![image-20220729233159000](C:\Users\zyb\AppData\Roaming\Typora\typora-user-images\image-20220729233159000.png)

   5. **ASKING命令**

      1. 作用：打开发送该命令客户端的`REDIS_ASKING`标识

      ![image-20220729234354171](C:\Users\zyb\AppData\Roaming\Typora\typora-user-images\image-20220729234354171.png)

      2. 当客户端接收到ASK错误并转向正在导入槽的节点时 客户端会先向节点发送一个ASKING命令 打开`REDIS_ASKING`标识 然后才重新发送想要执行的命令

   6. **ASK错误与MOVED错误的区别**

      ![image-20220729234927441](C:\Users\zyb\AppData\Roaming\Typora\typora-user-images\image-20220729234927441.png)

6. **复制与故障转移**

   1. 节点分为**主节点**和**从节点**

   2. 当主节点进入下线状态，集群中仍在运行的主节点将在该下线节点的从节点中选出一个节点作为新的主节点

   3. 设置从节点

      ```
      CLUSTER REPLICATE <node_id>
      ```

   4. **故障检测**

      1. **集群中每个节点会定期向其他节点发送PING消息 以此检测对方是否在线 如果接收PING消息的节点没有在规定时间内 向发送PING消息的源节点返回PONG消息 那么发送PING消息的节点就会将接收PING消息的节点标记为疑似下线**

      2. 集群中各个节点会通过互相发送消息的方式来交换集群中各个节点的状态信息

         ![image-20220730001406642](C:\Users\zyb\AppData\Roaming\Typora\typora-user-images\image-20220730001406642.png)

         ![image-20220730001340194](C:\Users\zyb\AppData\Roaming\Typora\typora-user-images\image-20220730001340194.png)

      3. **故障转移**

      ![image-20220730001513170](C:\Users\zyb\AppData\Roaming\Typora\typora-user-images\image-20220730001513170.png)

      

      4. **选举新的主节点**

         ![image-20220730001545825](C:\Users\zyb\AppData\Roaming\Typora\typora-user-images\image-20220730001545825.png)	![image-20220730001556109](C:\Users\zyb\AppData\Roaming\Typora\typora-user-images\image-20220730001556109.png)

         

7. **消息**

   1. PING
   2. PONG
   3. MEET
   4. FAIL
   5. PUBLISH